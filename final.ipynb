{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eve2024/Web_Programming/blob/main/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Dependencies"
      ],
      "metadata": {
        "id": "mmC6yuov_EZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, LSTM, Dense, Bidirectional, Attention\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ],
      "metadata": {
        "id": "vtHlgRFjnn-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset & Preprocess Data"
      ],
      "metadata": {
        "id": "WuMeeQjo_GPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "with open(\"/content/wikipedia.dat.txt\", \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "corrected_words = []\n",
        "misspelled_words = []\n",
        "\n",
        "for line in lines:\n",
        "    if line.startswith(\"$\"):\n",
        "        # Split the line into corrected and misspelled words\n",
        "        parts = line.strip().split(\"$\")\n",
        "        if len(parts) >= 2:  # Ensure there are at least two parts\n",
        "            corrected_word, *misspelled_word = parts[1].split()\n",
        "            corrected_words.append(corrected_word)\n",
        "            if misspelled_word:  # Check if misspelled_word is not empty\n",
        "                misspelled_words.append(misspelled_word[0])  # Extract the first element\n",
        "            else:\n",
        "                misspelled_words.append(\"\")  # Append an empty string if no misspelled word\n",
        "\n",
        "# Combine the corrected and misspelled words into pairs\n",
        "data = pd.DataFrame({\"corrected_word\": corrected_words, \"misspelled_word\": misspelled_words})\n",
        "\n",
        "# Data preprocessing\n",
        "vocab = set()\n",
        "for word in data[\"corrected_word\"].values:\n",
        "    vocab.update(list(word))\n",
        "for word in data[\"misspelled_word\"].values:\n",
        "    vocab.update(list(word))\n",
        "\n",
        "word_to_index = {char: index + 1 for index, char in enumerate(vocab)}\n"
      ],
      "metadata": {
        "id": "lO3DaTUW-BCF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "2111d06c-72bd-4051-ac0b-afe36f9ff42a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/wikipedia.dat.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a48487fa49a6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/wikipedia.dat.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcorrected_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/wikipedia.dat.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace Words with Integers"
      ],
      "metadata": {
        "id": "l0DgRfwE_KXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace words with integers in the dataset\n",
        "indexed_corrected_words = [[word_to_index[char] for char in word] for word in data[\"corrected_word\"].values]\n",
        "indexed_misspelled_words = [[word_to_index[char] for char in word] for word in data[\"misspelled_word\"].values]\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_seq_length = max(len(word) for word in indexed_corrected_words + indexed_misspelled_words)\n",
        "padded_corrected_words = pad_sequences(indexed_corrected_words, maxlen=max_seq_length)\n",
        "padded_misspelled_words = pad_sequences(indexed_misspelled_words, maxlen=max_seq_length)\n"
      ],
      "metadata": {
        "id": "j8h7TCkGtHFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into Training and Testing Sets"
      ],
      "metadata": {
        "id": "T4xA-AE9_NO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_misspelled_words, padded_corrected_words, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "qK4s6QkrnwDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bidirectional LSTM"
      ],
      "metadata": {
        "id": "rvRxLRZK_QbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Architecture\n",
        "input_layer = Input(shape=(max_seq_length,))\n",
        "embedding_layer = Embedding(input_dim=len(vocab) + 1, output_dim=100, input_length=max_seq_length)(input_layer)\n",
        "lstm_layer = Bidirectional(LSTM(64, return_sequences=True))(embedding_layer)\n",
        "attention = Attention()\n",
        "attention_output = attention([lstm_layer, lstm_layer])\n",
        "output_layer = Dense(len(vocab) + 1, activation='softmax')(attention_output)\n",
        "\n",
        "# Compile model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "guS1TOFin2Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the Model"
      ],
      "metadata": {
        "id": "NyLgir8p_Smb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "model.fit(X_train, y_train, batch_size=64, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "#model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "2Ccf_K_ln8D3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a18aa06-e55d-4fc4-e9df-002fc29410c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "25/25 [==============================] - 11s 222ms/step - loss: 3.2488 - accuracy: 0.4186 - val_loss: 2.3279 - val_accuracy: 0.4703\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 2s 92ms/step - loss: 2.0982 - accuracy: 0.4736 - val_loss: 1.8510 - val_accuracy: 0.5071\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 1.8884 - accuracy: 0.4938 - val_loss: 1.7981 - val_accuracy: 0.5071\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 3s 113ms/step - loss: 1.8645 - accuracy: 0.4931 - val_loss: 1.7889 - val_accuracy: 0.5071\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 1s 47ms/step - loss: 1.8559 - accuracy: 0.4927 - val_loss: 1.7859 - val_accuracy: 0.5071\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 1s 46ms/step - loss: 1.8553 - accuracy: 0.4917 - val_loss: 1.7829 - val_accuracy: 0.5086\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 1s 49ms/step - loss: 1.8498 - accuracy: 0.4926 - val_loss: 1.7871 - val_accuracy: 0.5086\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 1s 49ms/step - loss: 1.8589 - accuracy: 0.4914 - val_loss: 1.7970 - val_accuracy: 0.5071\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 1s 47ms/step - loss: 1.8536 - accuracy: 0.4932 - val_loss: 1.7801 - val_accuracy: 0.5071\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 1s 49ms/step - loss: 1.8439 - accuracy: 0.4937 - val_loss: 1.7811 - val_accuracy: 0.5086\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 1.8422 - accuracy: 0.4943\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ce7dac84fa0>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model Performance with Accuracy"
      ],
      "metadata": {
        "id": "vwQEs6Wj_Uee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt6Suk4Ix5Ol",
        "outputId": "e2eb8d59-836c-4e7d-aed3-5554d0194d8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 10ms/step - loss: 1.7756 - accuracy: 0.5071\n",
            "Test Accuracy: 0.5071428418159485\n",
            "13/13 [==============================] - 1s 8ms/step\n",
            "13/13 [==============================] - 0s 9ms/step\n"
          ]
        }
      ]
    }
  ]
}